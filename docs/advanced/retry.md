# Retry & Validation

A1 uses parallel candidates and retry strategies to ensure reliable LLM outputs, especially for structured output validation.

## RetryStrategy

`RetryStrategy` controls how many parallel attempts and sequential retries are used when LLM output validation fails.

### Default Configuration

**All LLM tools use RetryStrategy(max_iterations=3, num_candidates=3) by default:**

```python
from a1 import LLM

# Default: 3 parallel candidates × 3 retries each
llm = LLM("groq:openai/gpt-oss-20b")

# Equivalent to:
from a1 import RetryStrategy
llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(
        max_iterations=3,  # Retries per candidate
        num_candidates=3   # Parallel candidates
    )
)
```

### How It Works

When an LLM call with `output_schema` fails validation:

1. **Initial attempt (Candidate 0, Iteration 0)**:
   ```
   Try to validate initial response → Success? Return it!
   ```

2. **Parallel candidates** (if initial fails):
   ```
   Launch 3 parallel candidates:
     Candidate 0: Retry with stronger prompt (iteration 1, 2, ...)
     Candidate 1: New generation attempt (iteration 0, 1, 2, ...)
     Candidate 2: New generation attempt (iteration 0, 1, 2, ...)
   ```

3. **First success wins**:
   ```
   As soon as ANY candidate succeeds:
     - Return that result
     - Cancel remaining candidates
   ```

4. **Graceful degradation**:
   ```
   If all 3 × 3 = 9 attempts fail:
     - Return raw string content
     - Log warning
   ```

### Example Flow

```
INFO: Initial validation failed. Trying 3 candidates with 3 iterations each...
INFO: Candidate 0 iteration 0: Validation failed, retrying...
INFO: Candidate 1 iteration 0: Validation failed, retrying...
INFO: Candidate 2 iteration 0: Successfully validated
INFO: Got successful validation from one of 3 candidates
```

## Custom Retry Configuration

### Tune for Reliability vs Speed

```python
from a1 import LLM, RetryStrategy

# High reliability (for critical tasks)
critical_llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(
        max_iterations=5,  # More retries
        num_candidates=5   # More parallel attempts
    )
)
# Up to 5 × 5 = 25 attempts

# Fast execution (for simple tasks)
fast_llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(
        max_iterations=1,  # No retries
        num_candidates=1   # No parallel candidates
    )
)
# Only 1 attempt

# Balanced (default)
balanced_llm = LLM("groq:openai/gpt-oss-20b")
# 3 × 3 = 9 attempts
```

### When to Use What

| Use Case | max_iterations | num_candidates | Total Attempts |
|----------|---------------|----------------|----------------|
| Production JSON parsing | 5 | 5 | 25 |
| Default (recommended) | 3 | 3 | 9 |
| Development/testing | 2 | 2 | 4 |
| Simple prompts | 1 | 1 | 1 |

## Validation

### Output Schema Validation

When you specify `output_schema`, A1 validates the response against your Pydantic model:

```python
from pydantic import BaseModel, Field

class MathAnswer(BaseModel):
    answer: float = Field(..., ge=0)  # Must be >= 0
    steps: List[str] = Field(..., min_items=1)  # At least one step

llm = LLM("groq:openai/gpt-oss-20b")

# This will retry until valid or exhaust attempts
result = await llm.execute(
    content="Solve: 5 + 3",
    output_schema=MathAnswer
)
# result is guaranteed to be MathAnswer instance or raises error
```

### Validation Strategies

A1 tries multiple parsing strategies:

1. **Direct JSON parsing**:
   ```python
   parsed = json.loads(response)
   result = MathAnswer(**parsed)
   ```

2. **Single-field wrapping** (if schema has one field):
   ```python
   result = MathAnswer(answer=response_text)
   ```

3. **Primitive wrapping**:
   ```python
   parsed = json.loads(response)  # e.g., 42
   result = MathAnswer(answer=parsed)
   ```

### Retry with Stronger Prompts

On retry, A1 adds explicit validation instructions:

```
Your previous response could not be validated. 
Please respond with valid JSON matching this exact schema:
{
  "type": "object",
  "properties": {
    "answer": {"type": "number", "minimum": 0},
    "steps": {"type": "array", "items": {"type": "string"}}
  },
  "required": ["answer", "steps"]
}
```

## Code Generation Strategies

For AOT/JIT code generation, use `Strategy` (which extends `RetryStrategy`):

```python
from a1 import Strategy

strategy = Strategy(
    max_iterations=3,         # Refinement iterations per candidate
    num_candidates=3,         # Parallel code candidates
    min_candidates_for_comparison=2,  # Early stopping threshold
    accept_cost_threshold=5.0,        # Accept if cost < 5.0
    compare_cost_threshold=10.0       # Compare early if cost < 10.0
)

# Use in compilation
compiled = await runtime.aot(agent, strategy=strategy)
```

### Strategy Fields

| Field | Default | Description |
|-------|---------|-------------|
| `max_iterations` | 3 | Max refinement iterations per candidate |
| `num_candidates` | 1 | Number of parallel code candidates |
| `min_candidates_for_comparison` | 1 | Min valid candidates before comparison |
| `accept_cost_threshold` | None | Accept immediately if cost below this |
| `compare_cost_threshold` | None | Compare early when min_candidates met |

### How Code Generation Works

1. **Generate** `num_candidates` code versions in parallel
2. **Verify** each candidate with safety checks
3. **Estimate cost** for each valid candidate
4. **Select** lowest-cost candidate

```python
# Example with 3 candidates
strategy = Strategy(num_candidates=3)

# Runtime generates:
Candidate 0: cost = 15.0 (2 LLM calls)
Candidate 1: cost = 8.0  (1 LLM call)  ← Winner!
Candidate 2: cost = 22.0 (3 LLM calls)

# Selects candidate 1 (lowest cost)
```

## Error Handling

### Validation Failure

```python
from pydantic import ValidationError

try:
    result = await llm.execute(
        content="Give me data",
        output_schema=MySchema
    )
except ValidationError as e:
    print(f"All validation attempts failed: {e}")
    # Handle failure (all retries exhausted)
```

### Fallback to String

If `output_schema` validation fails for all attempts, A1 returns the raw string:

```python
# With default retry strategy (3×3)
result = await llm.execute(
    content="Give me JSON",
    output_schema=MySchema
)

# If all 9 attempts fail:
# result = "{broken json..." (raw string)
# WARNING logged: "All validation attempts failed (3 candidates × 3 iterations)"
```

## Observability

### Monitoring Retries

Enable info-level logging to see retry behavior:

```python
import logging
logging.basicConfig(level=logging.INFO)

result = await llm.execute(content="...", output_schema=Schema)

# Log output:
# INFO: Candidate 0 iteration 0: Successfully validated
# or
# INFO: Initial validation failed. Trying 3 candidates with 3 iterations each...
# INFO: Candidate 1 iteration 2: Successfully validated
# INFO: Got successful validation from one of 3 candidates
```

### Metrics Collection

```python
from a1 import LLM
import time

class InstrumentedLLM:
    """Track retry metrics."""
    
    def __init__(self, model: str):
        self.llm = LLM(model)
        self.total_attempts = 0
        self.successful_first_try = 0
        self.required_retries = 0
    
    async def execute(self, **kwargs):
        start = time.time()
        result = await self.llm.execute(**kwargs)
        duration = time.time() - start
        
        self.total_attempts += 1
        if duration < 1.0:  # Heuristic for first-try success
            self.successful_first_try += 1
        else:
            self.required_retries += 1
        
        return result
    
    def stats(self):
        return {
            "total": self.total_attempts,
            "first_try_success_rate": self.successful_first_try / self.total_attempts,
            "retry_rate": self.required_retries / self.total_attempts
        }
```

## Best Practices

### 1. Use Strong Schemas

```python
# ✅ Clear, validated schema
class Output(BaseModel):
    count: int = Field(..., ge=0, le=1000)
    items: List[str] = Field(..., min_items=1, max_items=10)

# ❌ Weak schema
class Output(BaseModel):
    data: Any  # Too permissive, defeats validation
```

### 2. Tune Retry for Task Complexity

```python
# Simple extraction: fewer retries
simple_llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(max_iterations=2, num_candidates=2)
)

# Complex JSON generation: more retries
complex_llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(max_iterations=5, num_candidates=3)
)
```

### 3. Monitor and Adjust

```python
# Start conservative
llm = LLM(
    "groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(max_iterations=5, num_candidates=3)
)

# Monitor logs for a week:
# - If always succeeds on first try → reduce to (2, 2)
# - If often retries → increase to (7, 5)
# - Adjust based on actual behavior
```

### 4. Use Appropriate Models

```python
# For complex structured output, use capable models
capable_llm = LLM(
    "gpt-4o",  # Better at following schemas
    retry_strategy=RetryStrategy(max_iterations=2, num_candidates=2)
)

# Less capable models may need more retries
budget_llm = LLM(
    "gpt-4o-mini",
    retry_strategy=RetryStrategy(max_iterations=5, num_candidates=3)
)
```

## See Also

- [LLM Integration](../guide/llm.md) - LLM tool usage
- [Strategies](strategies.md) - Code generation strategies
- [Agents](../guide/agents.md) - Agent configuration
