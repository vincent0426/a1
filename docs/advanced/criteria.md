# Evaluation Criteria

A1 provides two types of LLM-based evaluation criteria for assessing generated code:

- **QualitativeCriteria** - Boolean evaluation (pass/fail) using natural language criteria
- **QuantitativeCriteria** - Numeric scoring (e.g., 0-10) using natural language criteria

Both support **multiple samples** with parallel execution and **aggregation strategies** to improve reliability.

## Overview

Evaluation criteria allow you to use LLMs to judge code quality, complexity, readability, or any other aspect that's difficult to capture with traditional metrics. Instead of writing complex validation logic, you describe what you want in natural language.

### Key Concepts

1. **Natural Language Criteria**: Express evaluation goals in plain English
2. **Sampling**: Run multiple parallel evaluations for higher confidence
3. **Aggregation**: Combine multiple samples (voting, averaging, etc.)
4. **Integration**: Criteria integrate with `Strategy` for verification and cost estimation

## QualitativeCriteria

Returns a **boolean** (True/False) based on whether code meets qualitative criteria.

### Basic Usage

```python
from a1 import QualitativeCriteria, LLM, Agent, Runtime, Strategy

# Create LLM for evaluation
evaluator = LLM("gpt-4o-mini")

# Define qualitative criterion
is_readable = QualitativeCriteria(
    expression="Code is readable, well-structured, and follows Python best practices",
    llm=evaluator
)

# Use in verification strategy
strategy = Strategy(
    verify=is_readable,
    max_iterations=3,
    num_candidates=3
)

agent = Agent(
    output_schema=int,
    strategy=strategy
)

runtime = Runtime()
result = runtime.jit(agent, "Calculate fibonacci of 10")
```

### With Sampling

Run **multiple parallel evaluations** and use majority voting:

```python
# Run 5 parallel evaluations, require at least 4 to pass
is_high_quality = QualitativeCriteria(
    expression="Code is production-ready with proper error handling",
    llm=LLM("gpt-4o"),
    num_samples=5,           # 5 parallel LLM calls
    min_samples_for_aggregation=4,  # Need at least 4 successful responses
    min_pass=3               # Require 3 out of 5 to vote "pass"
)

strategy = Strategy(verify=is_high_quality)
```

### Parameters

- **expression** (str): Natural language description of what constitutes a "pass"
  - Example: `"Code is efficient and avoids unnecessary loops"`
  - Example: `"Code properly handles edge cases and errors"`
- **llm** (Tool): LLM tool to use for evaluation (e.g., `LLM("gpt-4o-mini")`)
- **num_samples** (int, default=1): Number of parallel evaluations to run
- **min_samples_for_aggregation** (int, default=1): Minimum successful responses needed
- **min_pass** (int, default=1): Number of "pass" votes needed (majority voting)

### How It Works

1. **Extraction**: Extracts generated Python code from the full output
2. **Prompt**: Creates evaluation prompt with your criteria
3. **Parallel Execution**: Runs `num_samples` LLM calls concurrently
4. **Voting**: Counts "yes"/"true"/"pass" responses
5. **Decision**: Returns True if `>= min_pass` samples voted "pass"

### Example: Multiple Criteria

```python
# Combine multiple qualitative checks
evaluator = LLM("gpt-4o-mini")

is_readable = QualitativeCriteria(
    expression="Code is readable and well-documented",
    llm=evaluator,
    num_samples=3,
    min_pass=2  # 2 out of 3 must agree
)

is_efficient = QualitativeCriteria(
    expression="Code uses efficient algorithms (no O(n²) or worse)",
    llm=evaluator,
    num_samples=3,
    min_pass=2
)

is_safe = QualitativeCriteria(
    expression="Code handles errors and edge cases properly",
    llm=evaluator,
    num_samples=3,
    min_pass=2
)

# Create verification function that checks all criteria
async def verify_all(code, agent):
    results = await asyncio.gather(
        is_readable.verify(code, agent),
        is_efficient.verify(code, agent),
        is_safe.verify(code, agent)
    )
    return all(results)  # All must pass

# Use in strategy
strategy = Strategy(verify=verify_all)
```

## QuantitativeCriteria

Returns a **numeric score** (e.g., 0-10) based on quantitative criteria.

### Basic Usage

```python
from a1 import QuantitativeCriteria, LLM, Agent, Runtime, Strategy

# Create LLM for scoring
scorer = LLM("gpt-4o-mini")

# Define quantitative criterion
complexity_score = QuantitativeCriteria(
    expression="How complex is this code? (0=very simple, 10=very complex)",
    llm=scorer,
    min=0.0,
    max=10.0,
    agg="avg"  # Average multiple samples
)

# Use in cost estimation strategy
strategy = Strategy(
    cost=complexity_score,
    max_iterations=3,
    num_candidates=3
)

agent = Agent(
    output_schema=str,
    strategy=strategy
)

runtime = Runtime()
result = runtime.jit(agent, "Parse a CSV file")
```

### With Sampling and Aggregation

Run **multiple parallel evaluations** and aggregate scores:

```python
# Run 5 parallel evaluations and take the median score
readability_score = QuantitativeCriteria(
    expression="Rate code readability from 0 (unreadable) to 10 (crystal clear)",
    llm=LLM("gpt-4o"),
    min=0.0,
    max=10.0,
    agg="med",               # Use median (robust to outliers)
    num_samples=5,           # 5 parallel LLM calls
    min_samples_for_aggregation=4  # Need at least 4 valid responses
)

strategy = Strategy(cost=readability_score)
```

### Parameters

- **expression** (str): Natural language description of what to score
  - Example: `"How many lines of code? (0=few, 10=many)"`
  - Example: `"Rate maintainability from 0 (unmaintainable) to 10 (highly maintainable)"`
- **llm** (Tool): LLM tool to use for scoring
- **min** (float, default=0.0): Minimum valid score
- **max** (float, default=10.0): Maximum valid score
- **agg** (str, default="avg"): Aggregation method for multiple samples
  - `"avg"` - Average (mean) of all valid scores
  - `"med"` - Median (middle value, robust to outliers)
  - `"min"` - Minimum (most conservative)
  - `"max"` - Maximum (most optimistic)
- **num_samples** (int, default=1): Number of parallel evaluations to run
- **min_samples_for_aggregation** (int, default=1): Minimum valid responses needed

### How It Works

1. **Extraction**: Extracts generated Python code from the full output
2. **Prompt**: Creates scoring prompt with your criteria and valid range
3. **Parallel Execution**: Runs `num_samples` LLM calls concurrently
4. **Validation**: Filters responses to valid numbers in `[min, max]` range
5. **Aggregation**: Combines valid scores using specified method
6. **Fallback**: Returns midpoint if insufficient valid samples

### Example: Multiple Metrics

```python
# Track multiple quantitative metrics
scorer = LLM("gpt-4o-mini")

complexity = QuantitativeCriteria(
    expression="Rate algorithmic complexity (0=O(1), 10=O(n³) or worse)",
    llm=scorer,
    min=0, max=10,
    agg="avg",
    num_samples=3
)

maintainability = QuantitativeCriteria(
    expression="Rate maintainability (0=unmaintainable, 10=very maintainable)",
    llm=scorer,
    min=0, max=10,
    agg="avg",
    num_samples=3
)

testability = QuantitativeCriteria(
    expression="Rate testability (0=hard to test, 10=easy to test)",
    llm=scorer,
    min=0, max=10,
    agg="avg",
    num_samples=3
)

# Create cost function that combines metrics
async def combined_cost(code, agent):
    scores = await asyncio.gather(
        complexity.compute(code, agent),
        maintainability.compute(code, agent),
        testability.compute(code, agent)
    )
    # Weighted combination
    return 0.4 * scores[0] + 0.3 * scores[1] + 0.3 * scores[2]

strategy = Strategy(cost=combined_cost)
```

## Integration with Strategy

Criteria are typically used as part of a `Strategy` for code generation:

```python
from a1 import Strategy, QualitativeCriteria, QuantitativeCriteria, LLM

evaluator = LLM("gpt-4o-mini")
scorer = LLM("gpt-4o-mini")

# Qualitative verification
is_valid = QualitativeCriteria(
    expression="Code is correct and handles edge cases",
    llm=evaluator,
    num_samples=3,
    min_pass=2
)

# Quantitative cost estimation
complexity = QuantitativeCriteria(
    expression="How complex is this code? (0=simple, 10=complex)",
    llm=scorer,
    min=0, max=10,
    agg="avg",
    num_samples=3
)

# Combine in strategy
strategy = Strategy(
    verify=is_valid,      # Boolean check
    cost=complexity,      # Numeric scoring
    max_iterations=3,     # Retry up to 3 times if verification fails
    num_candidates=3      # Generate 3 parallel candidates
)

agent = Agent(
    output_schema=dict,
    strategy=strategy
)
```

### Strategy Workflow with Criteria

1. **Generate**: Create `num_candidates` parallel code implementations
2. **Verify**: Run qualitative criteria on each candidate (if provided)
   - Filters out candidates that don't pass verification
3. **Cost**: Score remaining candidates using quantitative criteria (if provided)
   - Selects candidate with lowest cost
4. **Retry**: If no candidates pass verification, retry up to `max_iterations`
5. **Execute**: Run the winning candidate

## Aggregation Strategies

### For QualitativeCriteria (Voting)

- **min_pass**: Number of samples that must vote "pass"
- Example: `num_samples=5, min_pass=3` → majority voting (3 out of 5)
- Example: `num_samples=5, min_pass=5` → unanimous (all must agree)
- Example: `num_samples=5, min_pass=1` → at least one "pass" wins

### For QuantitativeCriteria (Numeric)

- **avg** (default): Mean of all valid scores
  - Use when: You want a balanced estimate
  - Example: `[3.0, 4.0, 5.0]` → `4.0`

- **med**: Median of all valid scores
  - Use when: You want robustness against outliers
  - Example: `[3.0, 4.0, 9.0]` → `4.0` (not skewed by the 9)

- **min**: Minimum (lowest) score
  - Use when: You want conservative estimates
  - Example: `[3.0, 4.0, 5.0]` → `3.0`

- **max**: Maximum (highest) score
  - Use when: You want optimistic estimates
  - Example: `[3.0, 4.0, 5.0]` → `5.0`

## Best Practices

### 1. Be Specific in Criteria

❌ **Vague**: `"Code is good"`
✅ **Specific**: `"Code follows PEP 8 style guidelines and includes type hints"`

❌ **Vague**: `"Rate the code"`
✅ **Specific**: `"Rate time complexity (0=O(1), 5=O(n), 10=O(n²) or worse)"`

### 2. Use Sampling for Important Decisions

```python
# Production code - use multiple samples
production_check = QualitativeCriteria(
    expression="Code is production-ready",
    llm=LLM("gpt-4o"),  # Use stronger model
    num_samples=5,      # More samples
    min_pass=4          # High bar
)

# Development code - single sample is fine
dev_check = QualitativeCriteria(
    expression="Code is basically correct",
    llm=LLM("gpt-4o-mini"),  # Faster/cheaper
    num_samples=1
)
```

### 3. Choose Appropriate Aggregation

```python
# For consistency metrics - use median (robust to outliers)
consistency = QuantitativeCriteria(
    expression="Rate code consistency (0=inconsistent, 10=very consistent)",
    llm=scorer,
    agg="med",  # Median handles outlier scores better
    num_samples=5
)

# For safety metrics - use min (conservative)
safety = QuantitativeCriteria(
    expression="Rate code safety (0=unsafe, 10=very safe)",
    llm=scorer,
    agg="min",  # Take the most conservative score
    num_samples=5
)

# For average metrics - use avg
complexity = QuantitativeCriteria(
    expression="Rate complexity (0=simple, 10=complex)",
    llm=scorer,
    agg="avg",  # Average gives balanced view
    num_samples=5
)
```

### 4. Set Appropriate Thresholds

```python
# Require minimum samples for aggregation
reliable_score = QuantitativeCriteria(
    expression="Rate code quality",
    llm=scorer,
    num_samples=5,
    min_samples_for_aggregation=4  # Need at least 4 valid responses
)

# If only 2 out of 5 LLM calls succeed, score defaults to midpoint
# This prevents decisions based on too few samples
```

### 5. Combine Qualitative and Quantitative

```python
# Use qualitative for hard constraints
passes_tests = QualitativeCriteria(
    expression="Code passes all test cases",
    llm=evaluator
)

# Use quantitative for optimization
is_efficient = QuantitativeCriteria(
    expression="Rate efficiency (0=slow, 10=fast)",
    llm=scorer
)

strategy = Strategy(
    verify=passes_tests,  # Must pass
    cost=is_efficient,    # Optimize among passing candidates
    num_candidates=5
)
```

### 6. Handle Edge Cases

```python
# Provide clear scoring guidelines
memory_usage = QuantitativeCriteria(
    expression="""Rate memory efficiency:
    0 = Minimal memory usage (< 1 MB)
    5 = Moderate memory usage (1-10 MB)
    10 = High memory usage (> 10 MB)
    
    Estimate based on data structures and allocations in the code.""",
    llm=scorer,
    min=0, max=10
)
```

## Common Use Cases

### 1. Code Quality Gates

```python
# Verify code meets quality standards
quality_gate = QualitativeCriteria(
    expression="""Code meets all of the following:
    - Follows PEP 8 style guidelines
    - Includes docstrings for functions
    - Has type hints
    - Handles errors properly
    - No security vulnerabilities""",
    llm=LLM("gpt-4o"),
    num_samples=3,
    min_pass=2
)
```

### 2. Complexity Estimation

```python
# Estimate implementation complexity
complexity_estimator = QuantitativeCriteria(
    expression="""Rate implementation complexity:
    0-2: Simple (basic operations, no loops)
    3-5: Moderate (simple loops, conditionals)
    6-8: Complex (nested loops, recursion)
    9-10: Very complex (dynamic programming, graph algorithms)""",
    llm=LLM("gpt-4o-mini"),
    min=0, max=10,
    agg="avg",
    num_samples=5
)
```

### 3. Readability Scoring

```python
# Score code readability
readability = QuantitativeCriteria(
    expression="""Rate code readability:
    10 = Self-documenting, crystal clear
    5 = Understandable with some effort
    0 = Confusing, hard to follow
    
    Consider: variable names, function names, structure, comments""",
    llm=LLM("gpt-4o"),
    min=0, max=10,
    agg="med",  # Median to avoid outliers
    num_samples=5
)
```

### 4. Test Coverage Assessment

```python
# Assess test coverage quality
test_coverage = QualitativeCriteria(
    expression="""Code includes tests for:
    - Normal/happy path cases
    - Edge cases (empty input, None, etc.)
    - Error cases (invalid input)
    - Boundary conditions""",
    llm=LLM("gpt-4o"),
    num_samples=3,
    min_pass=2
)
```

### 5. Performance Estimation

```python
# Estimate performance characteristics
performance = QuantitativeCriteria(
    expression="""Rate expected performance:
    0 = O(1) or O(log n) - excellent
    3 = O(n) - good
    6 = O(n log n) - acceptable
    9 = O(n²) - poor
    10 = O(2^n) or worse - very poor""",
    llm=LLM("gpt-4o"),
    min=0, max=10,
    agg="avg",
    num_samples=3
)
```

## Observability

### Monitoring Criteria Evaluations

```python
# Custom criteria with logging
class LoggedQuantitativeCriteria(QuantitativeCriteria):
    async def compute(self, code, agent):
        score = await super().compute(code, agent)
        print(f"[CRITERIA] {self.expression}: {score}/{self.max}")
        return score

complexity = LoggedQuantitativeCriteria(
    expression="Rate complexity",
    llm=scorer,
    min=0, max=10
)
```

### Tracking Sample Distributions

When using multiple samples, you might want to see the distribution:

```python
import statistics

# Subclass to track individual scores
class InstrumentedQuantitativeCriteria(QuantitativeCriteria):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sample_history = []
    
    async def compute(self, code, agent):
        # Call parent implementation
        score = await super().compute(code, agent)
        
        # Log distribution stats (you'd need to capture individual samples)
        print(f"Criteria: {self.expression}")
        print(f"Final score: {score}")
        print(f"Aggregation: {self.agg}")
        
        return score
```

## See Also

- [Retry and Validation](retry.md) - RetryStrategy and parallel candidates
- [Strategies](strategies.md) - Custom generation strategies
- [Runtime](../api/runtime.md) - Code execution and verification
- [LLM Integration](../guide/llm.md) - Using LLMs as tools

---

**Next Steps:**
- Learn about [custom strategies](strategies.md)
- Explore [runtime verification](../api/runtime.md)
- See [LLM integration](../guide/llm.md) for evaluator setup
